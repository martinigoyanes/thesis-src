You have the following GPUs: [0]
JOB:  364588
TASK: 
HOST: rivendell.rpl

Sat Mar 11 18:47:00 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:1A:00.0 Off |                  N/A |
| 27%   28C    P8    33W / 250W |      1MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
03/11/2023 18:47:26 - ERROR - pytorch_pretrained_bert.tokenization_openai -   Model name 'openai-gpt' was not found in model name list (openai-gpt). We assumed 'openai-gpt' was a path or url but couldn't find files https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json and https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt at this path or url.
03/11/2023 18:47:27 - ERROR - pytorch_pretrained_bert.modeling_openai -   Model name 'openai-gpt' was not found in model name list (openai-gpt). We assumed 'openai-gpt' was a path or url but couldn't find files https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin and https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json at this path or url.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Traceback (most recent call last):
  File "code/inference.py", line 188, in <module>
    main(args)
  File "code/inference.py", line 132, in main
    model = OriginalBlindGST.load_from_checkpoint(args.checkpoint_path)
  File "/Midgard/home/martinig/miniconda3/envs/thesis-src/lib/python3.8/site-packages/pytorch_lightning/core/saving.py", line 139, in load_from_checkpoint
    return _load_from_checkpoint(
  File "/Midgard/home/martinig/miniconda3/envs/thesis-src/lib/python3.8/site-packages/pytorch_lightning/core/saving.py", line 188, in _load_from_checkpoint
    return _load_state(cls, checkpoint, strict=strict, **kwargs)
  File "/Midgard/home/martinig/miniconda3/envs/thesis-src/lib/python3.8/site-packages/pytorch_lightning/core/saving.py", line 247, in _load_state
    keys = obj.load_state_dict(checkpoint["state_dict"], strict=strict)
  File "/Midgard/home/martinig/miniconda3/envs/thesis-src/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1671, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for OriginalBlindGST:
	Unexpected key(s) in state_dict: "model.transformer.tokens_embed.weight", "model.transformer.positions_embed.weight", "model.transformer.h.0.attn.bias", "model.transformer.h.0.attn.c_attn.weight", "model.transformer.h.0.attn.c_attn.bias", "model.transformer.h.0.attn.c_proj.weight", "model.transformer.h.0.attn.c_proj.bias", "model.transformer.h.0.ln_1.weight", "model.transformer.h.0.ln_1.bias", "model.transformer.h.0.mlp.c_fc.weight", "model.transformer.h.0.mlp.c_fc.bias", "model.transformer.h.0.mlp.c_proj.weight", "model.transformer.h.0.mlp.c_proj.bias", "model.transformer.h.0.ln_2.weight", "model.transformer.h.0.ln_2.bias", "model.transformer.h.1.attn.bias", "model.transformer.h.1.attn.c_attn.weight", "model.transformer.h.1.attn.c_attn.bias", "model.transformer.h.1.attn.c_proj.weight", "model.transformer.h.1.attn.c_proj.bias", "model.transformer.h.1.ln_1.weight", "model.transformer.h.1.ln_1.bias", "model.transformer.h.1.mlp.c_fc.weight", "model.transformer.h.1.mlp.c_fc.bias", "model.transformer.h.1.mlp.c_proj.weight", "model.transformer.h.1.mlp.c_proj.bias", "model.transformer.h.1.ln_2.weight", "model.transformer.h.1.ln_2.bias", "model.transformer.h.2.attn.bias", "model.transformer.h.2.attn.c_attn.weight", "model.transformer.h.2.attn.c_attn.bias", "model.transformer.h.2.attn.c_proj.weight", "model.transformer.h.2.attn.c_proj.bias", "model.transformer.h.2.ln_1.weight", "model.transformer.h.2.ln_1.bias", "model.transformer.h.2.mlp.c_fc.weight", "model.transformer.h.2.mlp.c_fc.bias", "model.transformer.h.2.mlp.c_proj.weight", "model.transformer.h.2.mlp.c_proj.bias", "model.transformer.h.2.ln_2.weight", "model.transformer.h.2.ln_2.bias", "model.transformer.h.3.attn.bias", "model.transformer.h.3.attn.c_attn.weight", "model.transformer.h.3.attn.c_attn.bias", "model.transformer.h.3.attn.c_proj.weight", "model.transformer.h.3.attn.c_proj.bias", "model.transformer.h.3.ln_1.weight", "model.transformer.h.3.ln_1.bias", "model.transformer.h.3.mlp.c_fc.weight", "model.transformer.h.3.mlp.c_fc.bias", "model.transformer.h.3.mlp.c_proj.weight", "model.transformer.h.3.mlp.c_proj.bias", "model.transformer.h.3.ln_2.weight", "model.transformer.h.3.ln_2.bias", "model.transformer.h.4.attn.bias", "model.transformer.h.4.attn.c_attn.weight", "model.transformer.h.4.attn.c_attn.bias", "model.transformer.h.4.attn.c_proj.weight", "model.transformer.h.4.attn.c_proj.bias", "model.transformer.h.4.ln_1.weight", "model.transformer.h.4.ln_1.bias", "model.transformer.h.4.mlp.c_fc.weight", "model.transformer.h.4.mlp.c_fc.bias", "model.transformer.h.4.mlp.c_proj.weight", "model.transformer.h.4.mlp.c_proj.bias", "model.transformer.h.4.ln_2.weight", "model.transformer.h.4.ln_2.bias", "model.transformer.h.5.attn.bias", "model.transformer.h.5.attn.c_attn.weight", "model.transformer.h.5.attn.c_attn.bias", "model.transformer.h.5.attn.c_proj.weight", "model.transformer.h.5.attn.c_proj.bias", "model.transformer.h.5.ln_1.weight", "model.transformer.h.5.ln_1.bias", "model.transformer.h.5.mlp.c_fc.weight", "model.transformer.h.5.mlp.c_fc.bias", "model.transformer.h.5.mlp.c_proj.weight", "model.transformer.h.5.mlp.c_proj.bias", "model.transformer.h.5.ln_2.weight", "model.transformer.h.5.ln_2.bias", "model.transformer.h.6.attn.bias", "model.transformer.h.6.attn.c_attn.weight", "model.transformer.h.6.attn.c_attn.bias", "model.transformer.h.6.attn.c_proj.weight", "model.transformer.h.6.attn.c_proj.bias", "model.transformer.h.6.ln_1.weight", "model.transformer.h.6.ln_1.bias", "model.transformer.h.6.mlp.c_fc.weight", "model.transformer.h.6.mlp.c_fc.bias", "model.transformer.h.6.mlp.c_proj.weight", "model.transformer.h.6.mlp.c_proj.bias", "model.transformer.h.6.ln_2.weight", "model.transformer.h.6.ln_2.bias", "model.transformer.h.7.attn.bias", "model.transformer.h.7.attn.c_attn.weight", "model.transformer.h.7.attn.c_attn.bias", "model.transformer.h.7.attn.c_proj.weight", "model.transformer.h.7.attn.c_proj.bias", "model.transformer.h.7.ln_1.weight", "model.transformer.h.7.ln_1.bias", "model.transformer.h.7.mlp.c_fc.weight", "model.transformer.h.7.mlp.c_fc.bias", "model.transformer.h.7.mlp.c_proj.weight", "model.transformer.h.7.mlp.c_proj.bias", "model.transformer.h.7.ln_2.weight", "model.transformer.h.7.ln_2.bias", "model.transformer.h.8.attn.bias", "model.transformer.h.8.attn.c_attn.weight", "model.transformer.h.8.attn.c_attn.bias", "model.transformer.h.8.attn.c_proj.weight", "model.transformer.h.8.attn.c_proj.bias", "model.transformer.h.8.ln_1.weight", "model.transformer.h.8.ln_1.bias", "model.transformer.h.8.mlp.c_fc.weight", "model.transformer.h.8.mlp.c_fc.bias", "model.transformer.h.8.mlp.c_proj.weight", "model.transformer.h.8.mlp.c_proj.bias", "model.transformer.h.8.ln_2.weight", "model.transformer.h.8.ln_2.bias", "model.transformer.h.9.attn.bias", "model.transformer.h.9.attn.c_attn.weight", "model.transformer.h.9.attn.c_attn.bias", "model.transformer.h.9.attn.c_proj.weight", "model.transformer.h.9.attn.c_proj.bias", "model.transformer.h.9.ln_1.weight", "model.transformer.h.9.ln_1.bias", "model.transformer.h.9.mlp.c_fc.weight", "model.transformer.h.9.mlp.c_fc.bias", "model.transformer.h.9.mlp.c_proj.weight", "model.transformer.h.9.mlp.c_proj.bias", "model.transformer.h.9.ln_2.weight", "model.transformer.h.9.ln_2.bias", "model.transformer.h.10.attn.bias", "model.transformer.h.10.attn.c_attn.weight", "model.transformer.h.10.attn.c_attn.bias", "model.transformer.h.10.attn.c_proj.weight", "model.transformer.h.10.attn.c_proj.bias", "model.transformer.h.10.ln_1.weight", "model.transformer.h.10.ln_1.bias", "model.transformer.h.10.mlp.c_fc.weight", "model.transformer.h.10.mlp.c_fc.bias", "model.transformer.h.10.mlp.c_proj.weight", "model.transformer.h.10.mlp.c_proj.bias", "model.transformer.h.10.ln_2.weight", "model.transformer.h.10.ln_2.bias", "model.transformer.h.11.attn.bias", "model.transformer.h.11.attn.c_attn.weight", "model.transformer.h.11.attn.c_attn.bias", "model.transformer.h.11.attn.c_proj.weight", "model.transformer.h.11.attn.c_proj.bias", "model.transformer.h.11.ln_1.weight", "model.transformer.h.11.ln_1.bias", "model.transformer.h.11.mlp.c_fc.weight", "model.transformer.h.11.mlp.c_fc.bias", "model.transformer.h.11.mlp.c_proj.weight", "model.transformer.h.11.mlp.c_proj.bias", "model.transformer.h.11.ln_2.weight", "model.transformer.h.11.ln_2.bias", "model.lm_head.decoder.weight". 
